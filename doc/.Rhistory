library(ggplot2)
library(ggrepel)
library(uwot)
install.packages('uwot')
library(word2vec)
library(tidytext)
library(dplyr)
library(ggplot2)
library(ggrepel)
library(uwot)
library(udpipe)
install.packages('udpipe')
library(word2vec)
library(tidytext)
library(dplyr)
library(ggplot2)
library(ggrepel)
library(uwot)
library(udpipe)
library(word2vec)
library(tidytext)
library(dplyr)
library(ggplot2)
library(ggrepel)
library(uwot)
library(udpipe)
pathfile <- '../output/processed_moments.csv'
cleaned_data <- read.csv(pathfile)
demo_data <- read.csv('https://raw.githubusercontent.com/megagonlabs/HappyDB/master/happydb/data/demographic.csv')
head(cleaned_data$cleaned_hm)
head(demo_data)
View(cleaned_data)
View(cleaned_data)
View(demo_data)
x <- tolower(cleaned_data$cleaned_hm)
cat(x[1])
process_data <- function(x, n_topics){
anno <- udpipe(x, "english", trace = 10, parallel.cores = 1)
anno <- subset(anno, !is.na(lemma) & nchar(lemma) > 1 & !upos %in% "PUNCT")
anno$text <- sprintf("%s//%s", anno$lemma, anno$upos)
x <- paste.data.frame(anno, term = "text", group = "doc_id", collapse = " ")
model <- word2vec(x = x$text, dim = n_topics, iter = 20, split = c(" ", ".\n?!"))
embedding <- as.matrix(model)
viz <- umap(embedding, n_neighbors = 15, n_threads = 2)
rownames(viz) <- rownames(embedding)
df <- data.frame(word = gsub("//.+", "", rownames(viz)),
upos = gsub(".+//", "", rownames(viz)),
x = viz[, 1], y = viz[, 2],
stringsAsFactors = FALSE)
df <- subset(df, upos %in% c("ADJ", "NOUN"))
return(list('data'=df,'model'=model))
}
l <- process_data(x, 15)
output_source <- function(code, context, ...){
rmarkdown::html_notebook_output_code("# R Logo")
}
output_file <-
rmarkdown::render(rmd_stub,
output_options = list(output_source = output_source),
quiet = TRUE)
rnotebook <- 'project_1.Rmd'
output_file <-
rmarkdown::render(rnotebook,
output_options = list(output_source = output_source),
quiet = TRUE)
output_file <-
rmarkdown::render(rnotebook,
output_options = list(output_source = output_source),
quiet = TRUE)
rm(output_source())
rm(output_source
py_install("socialsent",pip=TRUE)
install.packages('reticulate')
library(reticulate)
py_install("socialsent",pip=TRUE)
View(cleaned_data)
# Create corpus
corpus=Corpus(VectorSource(cleaned_data$text))
# Convert to lower-case
corpus=tm_map(corpus,tolower)
# Remove stopwords
corpus=tm_map(corpus,function(x) removeWords(x,stopwords()))
# convert corpus to a Plain Text Document
corpus=tm_map(corpus,PlainTextDocument)
col=brewer.pal(6,"Dark2")
detach("package:reticulate", unload = TRUE)
library(word2vec)
library(tidytext)
library(dplyr)
library(ggplot2)
library(ggrepel)
library(uwot)
library(udpipe)
library(ROAuth)
install.packages('ROAuth')
library(word2vec)
library(tidytext)
library(dplyr)
library(ggplot2)
library(ggrepel)
library(uwot)
library(udpipe)
library(ROAuth)
require(RCurl)
library(stringr)
library(tm)
library(ggmap)
install.packages('ggmap')
library(word2vec)
library(tidytext)
library(dplyr)
library(ggplot2)
library(ggrepel)
library(uwot)
library(udpipe)
library(ROAuth)
require(RCurl)
library(stringr)
library(tm)
library(ggmap)
library(plyr)
library(tm)
library(wordcloud)
install.packaes('wordcloud')
install.packages('wordcloud')
library(word2vec)
library(tidytext)
library(dplyr)
library(ggplot2)
library(ggrepel)
library(uwot)
library(udpipe)
library(ROAuth)
require(RCurl)
library(stringr)
library(tm)
library(ggmap)
library(plyr)
library(tm)
library(wordcloud)
# Create corpus
corpus=Corpus(VectorSource(cleaned_data$text))
# Convert to lower-case
corpus=tm_map(corpus,tolower)
# Remove stopwords
corpus=tm_map(corpus,function(x) removeWords(x,stopwords()))
# convert corpus to a Plain Text Document
corpus=tm_map(corpus,PlainTextDocument)
col=brewer.pal(6,"Dark2")
wordcloud(corpus, min.freq=25, scale=c(5,2),rot.per = 0.25,
random.color=T, max.word=45, random.order=F,colors=col)
rm(corpus)
rm(l)
l <- process_data(x, 15)
# Create corpus
corpus=Corpus(VectorSource(cleaned_data$text))
# convert corpus to a Plain Text Document
corpus=tm_map(corpus,PlainTextDocument)
col=brewer.pal(6,"Dark2")
wordcloud(corpus, min.freq=25, scale=c(5,2),rot.per = 0.25,
random.color=T, max.word=45, random.order=F,colors=col)
View(corpus)
install.packages('quanteda')
library(quanteda)
plot(quanteda::corpus(corpus), max.words = 50, random.order = FALSE)
detach("package:quanteda", unload = TRUE)
View(cleaned_data)
## Removing white space.
corpus <- tm_map(corpus, stripWhitespace)
## Converting to lower
corpus <- tm_map(corpus, tolower)
View(corpus)
corpus=tm_map(corpus,PlainTextDocument)
col=brewer.pal(6,"Dark2")
wordcloud(corpus, min.freq=25, scale=c(5,2),rot.per = 0.25,
random.color=T, max.word=45, random.order=F,colors=col)
rm(corpus)
View(cleaned_data)
# Create corpus
corpus=Corpus(VectorSource(cleaned_data$text))
# convert corpus to a Plain Text Document
corpus=tm_map(corpus,PlainTextDocument)
View(corpus)
# Create corpus
corpus=Corpus(VectorSource(corpus$tweet))
# Convert to lower-case
corpus=tm_map(corpus,tolower)
rm(corpus)
View(cleaned_data)
# Create corpus
corpus=Corpus(VectorSource(cleaned_data$original_hm))
# Convert to lower-case
corpus=tm_map(corpus,tolower)
# Remove stopwords
corpus=tm_map(corpus,function(x) removeWords(x,stopwords()))
# convert corpus to a Plain Text Document
corpus=tm_map(corpus,PlainTextDocument)
View(corpus)
col=brewer.pal(6,"Dark2")
wordcloud(corpus, min.freq=25, scale=c(5,2),rot.per = 0.25,
random.color=T, max.word=45, random.order=F,colors=col)
rm(corpus)
corpus=Corpus(VectorSource(cleaned_data$text))
# convert corpus to a Plain Text Document
corpus=tm_map(corpus,PlainTextDocument)
dtm <- TermDocumentMatrix(cleaned_data$text)
m <- as.matrix(dtm)
View(dtm)
m <- as.matrix(dtm)
library(word2vec)
library(tidytext)
library(dplyr)
library(ggplot2)
library(ggrepel)
library(uwot)
library(udpipe)
library(ROAuth)
require(RCurl)
library(stringr)
library(tm)
library(ggmap)
library(plyr)
library(tm)
library(wordcloud)
pathfile <- '../output/processed_moments.csv'
cleaned_data <- read.csv(pathfile)
demo_data <- read.csv('https://raw.githubusercontent.com/megagonlabs/HappyDB/master/happydb/data/demographic.csv')
head(cleaned_data$cleaned_hm)
head(demo_data)
View(cleaned_data)
## Per gender
gender_df <- merge(cleaned_data, demo_data)
View(gender_df)
dtm <- TermDocumentMatrix(cleaned_data$text)
m <- as.matrix(dtm)
library(usethis)
install.packages('usethis')
library(usethis)
usethis::edit_r_environ()
system("./.Renviron")
dtm <- TermDocumentMatrix(cleaned_data$text)
m <- as.matrix(dtm)
Sys.setenv('R_MAX_VSIZE'=32000000000)
dtm <- TermDocumentMatrix(cleaned_data$text)
m <- as.matrix(dtm)
sessionInfo()
#for some reason this code doesn't work unless I execute the steps in this stackoverflow link: https://stackoverflow.com/questions/51295402/r-on-macos-error-vector-memory-exhausted-limit-reached
dtm <- TermDocumentMatrix(cleaned_data$text)
library(word2vec)
library(tidytext)
library(tidyverse)
library(dplyr)
library(ggplot2)
library(ggrepel)
library(plyr)
library(tm)
library(wordcloud)
#Most frequent terms for females discussing happy moments:
dtm_f <- TermDocumentMatrix(f_1$cleaned_hm)
#FEMALE vs MALE
f_1 <- subset(gender_df, gender_df$gender == 'f')
m_2 <- subset(gender_df, gender_df$gender == 'm')
#Most frequent terms for females discussing happy moments:
dtm_f <- TermDocumentMatrix(f_1$cleaned_hm)
m_f <- as.matrix(dtm_f)
v_f <- sort(rowSums(m_f),decreasing=TRUE)
d_f <- data.frame(word = names(v_f),freq=v_f)
head(d_f, 10)
#Most frequent terms for females discussing happy moments:
dtm_f <- TermDocumentMatrix(f_1$text)
m_f <- as.matrix(dtm_f)
v_f <- sort(rowSums(m_f),decreasing=TRUE)
d_f <- data.frame(word = names(v_f),freq=v_f)
head(d_f, 10)
#Most frequent terms for males discussing happy moments:
dtm_m <- TermDocumentMatrix(m_2$text)
m_m <- as.matrix(dtm_m)
v_m <- sort(rowSums(m_m),decreasing=TRUE)
d_m <- data.frame(word = names(v_m),freq=v_m)
head(d_m, 10)
love <- c('wife', 'husband', 'kiss', 'date', 'boyfriend', 'girlfriend', 'fiance', 'fiancee', 'engaged', 'sex', 'sexual', 'intercourse', 'dating', 'romance', 'romantic', 'spouse', 'partner', 'lover')
romance_male <- subset(d_m, word %in% love)
View(romance_male)
romance_female <- subset(d_f, word %in% love)
View(romance_female)
romance_male <- subset(d_m, word %in% love)
barplot(romance_male$freq, las = 2, names.arg = romance_male$word,
col ="lightblue", main ="Romance-related words: Male",
ylab = "Word frequencies")
romance_female <- subset(d_f, word %in% love)
barplot(romance_female$freq, las = 2, names.arg = romance_female$word,
col ="lightblue", main ="Romance-related words: Female",
ylab = "Word frequencies")
romance_df <- merge(romance_male, romance_female)
View(romance_df)
romance_df <- merge(romance_male, romance_female, by = 'row.names', all=TRUE)
View(romance_df)
romance_df <- merge(romance_male, romance_female, by = 'row.names')
View(romance_df)
romance_df <- merge(romance_male, romance_female, all = TRUE)
View(romance_df)
romance_df <- merge(romance_male, romance_female, by = 'word')
View(romance_df)
View(romance_male)
View(romance_df)
romance_df <- merge(romance_male, romance_female, by = 'word') %>%
rename(freq.x = Male, freq.y = Female)
romance_df <- merge(romance_male, romance_female, by = 'word') %>%
rename(Male = freq.x, Female = freq.y)
View(romance_df)
romance_df <- merge(romance_male, romance_female, by = 'word') %>%
rename('Male' = freq.x, 'Female' = freq.y)
romance_df <- merge(romance_male, romance_female, by = 'word') %>%
rename(freq.x = 'Male', freq.y = 'Female')
View(romance_df)
romance_df %>%
rename(freq.x = 'Male', freq.y = 'Female')
romance_df %>%
rename(Male = freq.x)
View(romance_df)
romance_df %>%
dplyr::rename(Male = freq.x)
rm(romance_df)
romance_df <- merge(romance_male, romance_female, by = 'word') %>%
dplyr::rename(Male = freq.x, Female = freq.y)
View(romance_df)
View(gender_df)
View(romance_female)
View(romance_df)
specie <- c(rep("sorgho" , 3) , rep("poacee" , 3) , rep("banana" , 3) , rep("triticum" , 3) )
condition <- rep(c("normal" , "stress" , "Nitrogen") , 4)
value <- abs(rnorm(12 , 0 , 15))
data <- data.frame(specie,condition,value)
View(data)
rm(specie)
rm(condition)
rm(value)
View(data)
View(romance_df)
View(romance_female)
rm(data)
romance_male <- subset(d_m, word %in% love)
romance_male$Gender <- 'Male'
romance_female <- subset(d_f, word %in% love)
romance_female$Gender <- 'Female'
View(romance_female)
View(romance_male)
romance_df <- merge(romance_male, romance_female, by = 'word')
View(romance_df)
romance_df <- merge(romance_male, romance_female, by = 'col.names')
romance_df <- merge(romance_male, romance_female)
View(romance_df)
romance_df <- merge(romance_male, romance_female, by = 'word', all = TRUE)
View(romance_df)
View(romance_female)
romance_df <- merge(romance_male, romance_female, by = 'word')
View(romance_df)
romance_df <- merge(romance_male, romance_female, by = 'row.names')
View(romance_df)
romance_df <- rbind(romance_male, romance_female)
View(romance_df)
ggplot(romance_df, aes(fill=Gender, y=freq, x=word)) +
geom_bar(position="dodge", stat="identity")
#for some reason this code doesn't work unless I execute the steps in this stackoverflow link: https://stackoverflow.com/questions/51295402/r-on-macos-error-vector-memory-exhausted-limit-reached
dtm <- TermDocumentMatrix(cleaned_data$text)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
romance <- subset(d, word %in% love)
View(romance)
View(romance)
View(romance_male)
romance_male$avg_freq <- romance_male$freq / romance$freq
View(romance_male)
View(romance_male)
romance_male$avg_freq <- romance_male$freq / romance$freq
romance_female$avg_freq <- romance_female$freq / romance$freq
View(romance_female)
romance_f <- romance(!word == 'intercourse')
romance_f <- romance[!word == 'intercourse']
romance_f <- romance[!word %in% 'intercourse']
romance_f <- romance[!word %in% c('intercourse')]
View(romance)
romance_f <- romance[romance$word != 'intercourse')]
romance_f <- romance[romance$word != 'intercourse']
romance_f <- romance[romance$word != 'intercourse',]
View(romance_f)
romance_female$avg_freq <- romance_female$freq / romance_f$freq
View(romance_female)
romance_df <- rbind(romance_male, romance_female)
ggplot(romance_df, aes(fill=Gender, y=freq, x=word)) +
geom_bar(position="dodge", stat="identity")
romance_df <- rbind(romance_male, romance_female)
ggplot(romance_df, aes(fill=Gender, y=avg_freq, x=word)) +
geom_bar(position="dodge", stat="identity")
View(romance_df)
love <- c('wife', 'husband', 'kiss', 'date', 'boyfriend', 'girlfriend', 'fiance', 'fiancee', 'engaged', 'sex', 'sexual', 'dating', 'romance', 'romantic', 'spouse', 'partner', 'lover')
#Overall frequency of romance across both categories
romance <- subset(d, word %in% love)
#Subsetting by gender
romance_male <- subset(d_m, word %in% love)
romance_male$Gender <- 'Male'
romance_female <- subset(d_f, word %in% love)
romance_female$Gender <- 'Female'
#Compute average frequency for word for each gender category by dividing frequency for gender category by overall frequency over all documents
romance_male$avg_freq <- romance_male$freq / romance$freq
romance_female$avg_freq <- romance_female$freq / romance$freq
romance_df <- rbind(romance_male, romance_female)
ggplot(romance_df, aes(fill=Gender, y=avg_freq, x=word)) +
geom_bar(position="dodge", stat="identity")
library(word2vec)
library(tidytext)
library(dplyr)
library(ggplot2)
library(ggrepel)
library(uwot)
library(udpipe)
#pathfile<-paste0(getwd(),'/output/processed_moments.csv')
pathfile <- '/Users/clairehe/Documents/GitHub/ADS_Teaching/Projects_StarterCodes/Project1-RNotebook/output/processed_moments.csv'
cleaned_data <- read.csv(pathfile)
pathfile <- '../output/processed_moments.csv'
cleaned_data <- read.csv(pathfile)
demo_data <- read.csv('https://raw.githubusercontent.com/megagonlabs/HappyDB/master/happydb/data/demographic.csv')
head(cleaned_data$cleaned_hm)
head(demo_data)
x <- tolower(cleaned_data$cleaned_hm)
cat(x[1])
process_data <- function(x, n_topics){
anno <- udpipe(x, "english", trace = 10, parallel.cores = 1)
anno <- subset(anno, !is.na(lemma) & nchar(lemma) > 1 & !upos %in% "PUNCT")
anno$text <- sprintf("%s//%s", anno$lemma, anno$upos)
x <- paste.data.frame(anno, term = "text", group = "doc_id", collapse = " ")
model <- word2vec(x = x$text, dim = n_topics, iter = 20, split = c(" ", ".\n?!"))
embedding <- as.matrix(model)
viz <- umap(embedding, n_neighbors = 15, n_threads = 2)
rownames(viz) <- rownames(embedding)
df <- data.frame(word = gsub("//.+", "", rownames(viz)),
upos = gsub(".+//", "", rownames(viz)),
x = viz[, 1], y = viz[, 2],
stringsAsFactors = FALSE)
df <- subset(df, upos %in% c("ADJ", "NOUN"))
return(list('data'=df,'model'=model))
}
l <- process_data(x, 15)
animal_df <- subset(l$data, rownames(l$data) %in% word_list_animal)
lookslike2 <- predict(l$model, c("animal//NOUN"), type = "nearest", top_n = 10)
word_list_animal <- lookslike2$`animal//NOUN`$term2
word_list_happiness <- predict(l$model, c("happy//ADJ"), type = "nearest", top_n = 50)$`happy//ADJ`$term2
animal_df <- subset(l$data, rownames(l$data) %in% word_list_animal)
happiness_df <- subset(l$data, rownames(l$data) %in% word_list_happiness)
View(animal_df)
View(happiness_df)
View(l)
#  Overall topics
# compute document term matrix with terms >= minimumFrequency
minimumFrequency <- 5
DTM <- DocumentTermMatrix(cleaned_data$text, control = list(bounds = list(global = c(minimumFrequency, Inf))))
library(word2vec)
library(uwot)
library(tidytext)
library(tidyverse)
library(dplyr)
library(ggplot2)
library(ggrepel)
library(tm)
library(wordcloud)
library(udpipe)
#for topic modeling
library(topicmodels)
library(reshape2)
library(pals)
library(SnowballC)
library(lda)
library(ldatuning)
library(kableExtra)
library(DT)
library(flextable)
pathfile <- '../output/processed_moments.csv'
cleaned_data <- read.csv(pathfile)
demo_data <- read.csv('../data/demographic.csv')
head(cleaned_data$cleaned_hm)
head(demo_data)
#for some reason this code doesn't work unless I execute the steps in this stackoverflow link: https://stackoverflow.com/questions/51295402/r-on-macos-error-vector-memory-exhausted-limit-reached
dtm <- TermDocumentMatrix(cleaned_data$text)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
setwd('Desktop/GitHub/ads-fall2023-project2-group2/doc/')
getwd()
ls
setwd('Desktop/GitHub/ads-fall2023-project2-group2/doc/')
library(word2vec)
library(uwot)
library(tidytext)
library(tidyverse)
library(dplyr)
library(ggplot2)
library(ggrepel)
library(tm)
library(wordcloud)
library(udpipe)
#for sentiment analysis
library(syuzhet)
library(lubridate)
library(scales)
library(reshape2)
pathfile <- '../data/WeatherNYC.csv'
weather_data <- read.csv(pathfile)
pathfile_2 <- '../data/citibike_data.csv'
citibike_data <- read.csv(pathfile_2)
